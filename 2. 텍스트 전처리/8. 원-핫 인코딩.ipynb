{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"8. 원-핫 인코딩.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyNzvptWh0QMX0z2s7rg5MqI"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["### **단어 집합(Vocabulary)**  \n","- 단어 집합은 서로 다른 단어들의 집합이다.\n","- 기본적으로 book과 books와 같이 단어의 변형 형태도 다른 단어로 간주한다.\n","- 원-핫 인코딩을 위해서 먼저 해야하 일은 단어 집합을 만드는 일이다.\n","- 텍스트의 모든 단어를 중복을 허용하지 않고 모아둔 집합\n","- 단어 집합을 만든 후 이 단어 집합에 고유한 정수를 부여하는 정수 인코딩을 진행한다."],"metadata":{"id":"3_rckefKgM8F"}},{"cell_type":"markdown","source":["# **원-핫 인코딩(One-Hot Encoding)**\n","단어 집합의 크기를 벡터의 차원으로 하고, 표현하고 싶은 단어의 인덱스에 1의 값을 부여하고, 다른 인덱스에는 0을 부여하는 단어의 벡터 표현 방식"],"metadata":{"id":"qt1PQp8-hHFn"}},{"cell_type":"markdown","source":["**두 가지 과정**  \n","1. 정수 인코딩을 수행 -> 각 단어에 고유한 정수를 부여\n","2. 표현하고 싶은 단어의 고유한 정수를 인덱스로 간주하고 해당 위치에 1을 부여하고, 다른 단어의 인덱스의 위치에는 0을 부여"],"metadata":{"id":"HyBf700hhZET"}},{"cell_type":"code","source":["!pip install konlpy"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"QVrC0nWiiL6L","executionInfo":{"status":"ok","timestamp":1652967405282,"user_tz":-540,"elapsed":5432,"user":{"displayName":"남희정","userId":"05541235909914296292"}},"outputId":"d2da73fa-1e90-4942-b1a2-0cccb564dfc9"},"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting konlpy\n","  Downloading konlpy-0.6.0-py2.py3-none-any.whl (19.4 MB)\n","\u001b[K     |████████████████████████████████| 19.4 MB 7.2 MB/s \n","\u001b[?25hRequirement already satisfied: numpy>=1.6 in /usr/local/lib/python3.7/dist-packages (from konlpy) (1.21.6)\n","Collecting JPype1>=0.7.0\n","  Downloading JPype1-1.3.0-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.whl (448 kB)\n","\u001b[K     |████████████████████████████████| 448 kB 45.8 MB/s \n","\u001b[?25hRequirement already satisfied: lxml>=4.1.0 in /usr/local/lib/python3.7/dist-packages (from konlpy) (4.2.6)\n","Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from JPype1>=0.7.0->konlpy) (4.2.0)\n","Installing collected packages: JPype1, konlpy\n","Successfully installed JPype1-1.3.0 konlpy-0.6.0\n"]}]},{"cell_type":"code","execution_count":3,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"JudRUQXJgIr-","executionInfo":{"status":"ok","timestamp":1652967415506,"user_tz":-540,"elapsed":10231,"user":{"displayName":"남희정","userId":"05541235909914296292"}},"outputId":"773e8036-5db3-4940-df07-128f23540bc7"},"outputs":[{"output_type":"stream","name":"stdout","text":["['나', '는', '자연어', '처리', '를', '배운다']\n"]}],"source":["# Okt 형태소 분석기를 통해서 문장에 대해서 토큰화 수행\n","from konlpy.tag import Okt\n","\n","okt = Okt()\n","tokens = okt.morphs(\"나는 자연어 처리를 배운다\")\n","print(tokens)"]},{"cell_type":"code","source":["# 각 토큰에 대해서 고유한 정수를 부여\n","word_to_index = {word : index for index, word in enumerate(tokens)}\n","print(\"단어 집합 :\", word_to_index)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"tBykzo7jh9VA","executionInfo":{"status":"ok","timestamp":1652967552441,"user_tz":-540,"elapsed":18,"user":{"displayName":"남희정","userId":"05541235909914296292"}},"outputId":"b8d7a854-251b-41b3-d771-4f62be9d7d68"},"execution_count":4,"outputs":[{"output_type":"stream","name":"stdout","text":["단어 집합 : {'나': 0, '는': 1, '자연어': 2, '처리': 3, '를': 4, '배운다': 5}\n"]}]},{"cell_type":"markdown","source":["지금은 문장이 짧기 때문에 각 단어의 빈도수를 고려하지 않지만, 빈도수 순으로 단어를 정렬하여 정수를 부여하는 경우가 많다"],"metadata":{"id":"PmcRGxO8ie8W"}},{"cell_type":"code","source":["# 원-핫 벡터를 만드는 함수\n","def one_hot_encoding(word, word_to_index):\n","  one_hot_vector = [0]*(len(word_to_index))\n","  index = word_to_index[word]\n","  one_hot_vector[index] = 1\n","  return one_hot_vector"],"metadata":{"id":"MkaMCNLPif1w","executionInfo":{"status":"ok","timestamp":1652967646258,"user_tz":-540,"elapsed":11,"user":{"displayName":"남희정","userId":"05541235909914296292"}}},"execution_count":5,"outputs":[]},{"cell_type":"code","source":["# \"자연어\"라는 단어의 원-핫 벡터\n","print(\"자연어의 index :\", word_to_index[\"자연어\"])\n","one_hot_encoding(\"자연어\", word_to_index)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"dlfD4RmjjJ-i","executionInfo":{"status":"ok","timestamp":1652967729577,"user_tz":-540,"elapsed":428,"user":{"displayName":"남희정","userId":"05541235909914296292"}},"outputId":"6f329320-0d72-4eea-f304-178b26c15691"},"execution_count":7,"outputs":[{"output_type":"stream","name":"stdout","text":["자연어의 index : 2\n"]},{"output_type":"execute_result","data":{"text/plain":["[0, 0, 1, 0, 0, 0]"]},"metadata":{},"execution_count":7}]},{"cell_type":"markdown","source":["# 케라스를 이용한 원-핫 인코딩"],"metadata":{"id":"al6HKJxNjgre"}},{"cell_type":"markdown","source":["케라스는 원-핫 인코딩을 수행하는 유용한 도구 to_categorical()를 지원한다."],"metadata":{"id":"b9S8kEzZjlxM"}},{"cell_type":"code","source":["text = \"나랑 점심 먹으러 갈래 점심 메뉴는 햄버거 갈래 갈래 햄버거 최고야\""],"metadata":{"id":"foHYVuW5jarZ","executionInfo":{"status":"ok","timestamp":1652967789806,"user_tz":-540,"elapsed":12,"user":{"displayName":"남희정","userId":"05541235909914296292"}}},"execution_count":8,"outputs":[]},{"cell_type":"code","source":["# 패키지 임포트\n","from tensorflow.keras.preprocessing.text import Tokenizer\n","from tensorflow.keras.utils import to_categorical"],"metadata":{"id":"44HxjqcbjtDh","executionInfo":{"status":"ok","timestamp":1652967817070,"user_tz":-540,"elapsed":6513,"user":{"displayName":"남희정","userId":"05541235909914296292"}}},"execution_count":9,"outputs":[]},{"cell_type":"code","source":["# 정수 인코딩\n","tokenizer = Tokenizer()\n","tokenizer.fit_on_texts([text])\n","print(\"단어 집합 :\", tokenizer.word_index)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"v73j0JuvjyOK","executionInfo":{"status":"ok","timestamp":1652967860517,"user_tz":-540,"elapsed":12,"user":{"displayName":"남희정","userId":"05541235909914296292"}},"outputId":"3bbe4b16-550c-44c9-9d0d-a5f5675816cb"},"execution_count":10,"outputs":[{"output_type":"stream","name":"stdout","text":["단어 집합 : {'갈래': 1, '점심': 2, '햄버거': 3, '나랑': 4, '먹으러': 5, '메뉴는': 6, '최고야': 7}\n"]}]},{"cell_type":"code","source":["# text_to_sequences()를 통해서 정수 시퀀스로 변환\n","sub_text = \"점심 먹으러 갈래 메뉴는 햄버거 최고야\"\n","encoded = tokenizer.texts_to_sequences([sub_text])[0]\n","print(encoded)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"1oDTLXWyj-cn","executionInfo":{"status":"ok","timestamp":1652967952805,"user_tz":-540,"elapsed":478,"user":{"displayName":"남희정","userId":"05541235909914296292"}},"outputId":"2f10d987-8475-4ae3-bbd7-5985f0dc99bf"},"execution_count":12,"outputs":[{"output_type":"stream","name":"stdout","text":["[2, 5, 1, 6, 3, 7]\n"]}]},{"cell_type":"code","source":["one_hot = to_categorical(encoded)\n","print(one_hot)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"U7zvKpqlkTQ2","executionInfo":{"status":"ok","timestamp":1652967993366,"user_tz":-540,"elapsed":5,"user":{"displayName":"남희정","userId":"05541235909914296292"}},"outputId":"ef69c7a8-018c-4efa-eb41-02116982adb1"},"execution_count":13,"outputs":[{"output_type":"stream","name":"stdout","text":["[[0. 0. 1. 0. 0. 0. 0. 0.]\n"," [0. 0. 0. 0. 0. 1. 0. 0.]\n"," [0. 1. 0. 0. 0. 0. 0. 0.]\n"," [0. 0. 0. 0. 0. 0. 1. 0.]\n"," [0. 0. 0. 1. 0. 0. 0. 0.]\n"," [0. 0. 0. 0. 0. 0. 0. 1.]]\n"]}]},{"cell_type":"markdown","source":["### **원-핫 인코딩의 한계**\n","1. 단어의 개수가 늘어날수록, 벡터를 저장하기 위해 필요한 공간이 계속 늘어난다는 단점이 있다.\n","  - 원 핫 벡터는 단어 집합의 크기가 곧 벡터의 차원 수가 된다.\n","  - 이는 저장 공간 측면에서는 매우 비효율적인 표현 방법이다.\n","2. 단어의 유사도를 표현하지 못한다는 단점이 있다.\n","  - 단어 간 유사성을 알 수 없다는 단점은 검색 시스템 등에서는 문제가 될 소지가 있다."],"metadata":{"id":"tEcIyxPAkkxn"}},{"cell_type":"markdown","source":["### **한계를 극복하기 위한 해결 방법**\n","1. 카운트 기반의 벡터화 방법인 LSA(잠재 의미 분석), HAL 등이 있다.\n","2. 예측 기반으로 벡터화하는 NNLM, RNNLM, Word2Vec, FastText 등이 있다.\n","3. 카운트 기반과 예측 기반 두 가지 방법을 모두 사용하는 방법은 GloVe라는 방법이 존재한다."],"metadata":{"id":"1woQGte4lPnj"}}]}